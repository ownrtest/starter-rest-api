{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXUHUdUsddee"
      },
      "source": [
        "# Fish Speech"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y5IgX9tddee"
      },
      "source": [
        "### For Windows User / win用户"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "bat"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ndMQlQcddef",
        "outputId": "c314ce71-9abb-4034-c119-05c6e0d835c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fish-speech'...\n",
            "remote: Enumerating objects: 5490, done.\u001b[K\n",
            "remote: Counting objects: 100% (1577/1577), done.\u001b[K\n",
            "remote: Compressing objects: 100% (333/333), done.\u001b[K\n",
            "remote: Total 5490 (delta 1331), reused 1244 (delta 1244), pack-reused 3913 (from 3)\u001b[K\n",
            "Receiving objects: 100% (5490/5490), 18.59 MiB | 17.50 MiB/s, done.\n",
            "Resolving deltas: 100% (3643/3643), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/fishaudio/fish-speech.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnjq3NFKddef"
      },
      "source": [
        "### For Linux User / Linux 用户"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 更改工作目录\n",
        "os.chdir('/content/fish-speech')\n",
        "\n",
        "# 检查当前工作目录\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCr-dX12d8-O",
        "outputId": "716925ca-964d-4311-9018-b333ff3356b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fish-speech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe42NYkXe3QI",
        "outputId": "ae5406a2-e298-4d90-c1c6-b4d32d51da0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.1\n",
            "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19.1\n",
            "  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting torchaudio==2.4.1\n",
            "  Downloading torchaudio-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.1)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.1) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.19.1) (11.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.1) (1.3.0)\n",
            "Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.4.1-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu124\n",
            "    Uninstalling torchvision-0.20.1+cu124:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.5.1+cu124\n",
            "    Uninstalling torchaudio-2.5.1+cu124:\n",
            "      Successfully uninstalled torchaudio-2.5.1+cu124\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 torchaudio-2.4.1 torchvision-0.19.1 triton-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libsox-dev ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHA7iythe7or",
        "outputId": "c0dec459-d3ce-4747-aa81-f86e2da71d9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-all\n",
            "  libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3 libsox-fmt-oss libsox-fmt-pulse\n",
            "  libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libaudio2 libsndio6.1\n",
            "The following NEW packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0 libsox-dev\n",
            "  libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3 libsox-fmt-oss\n",
            "  libsox-fmt-pulse libsox3 libwavpack1\n",
            "0 upgraded, 16 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,053 kB of archives.\n",
            "After this operation, 4,061 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao-common all 1.2.2+20180113-1.1ubuntu3 [6,568 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao4 amd64 1.2.2+20180113-1.1ubuntu3 [35.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libid3tag0 amd64 0.15.1b-14 [31.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmad0 amd64 0.15.1b-10ubuntu1 [63.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-ao amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,740 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [17.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-oss amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [9,424 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-pulse amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,732 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-all amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [5,016 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-dev amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [356 kB]\n",
            "Fetched 1,053 kB in 1s (1,411 kB/s)\n",
            "Selecting previously unselected package libao-common.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libao-common_1.2.2+20180113-1.1ubuntu3_all.deb ...\n",
            "Unpacking libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libao4:amd64.\n",
            "Preparing to unpack .../01-libao4_1.2.2+20180113-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../02-libid3tag0_0.15.1b-14_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../03-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../04-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../05-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-ao:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-ao_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../09-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../11-libsox-fmt-mp3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-oss:amd64.\n",
            "Preparing to unpack .../12-libsox-fmt-oss_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-pulse:amd64.\n",
            "Preparing to unpack .../13-libsox-fmt-pulse_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-all:amd64.\n",
            "Preparing to unpack .../14-libsox-fmt-all_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-dev:amd64.\n",
            "Preparing to unpack .../15-libsox-dev_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install build-essential \\\n",
        "    cmake \\\n",
        "    libasound-dev \\\n",
        "    portaudio19-dev \\\n",
        "    libportaudio2 \\\n",
        "    libportaudiocpp0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-GRtc8xfCUw",
        "outputId": "b68391c3-b8ff-49b3-ca37-29f6b6d81b92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'libasound2-dev' instead of 'libasound-dev'\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libasound2-dev is already the newest version (1.2.6.1-1ubuntu1).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 1s (330 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 125096 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C-MzkWTyfHdn",
        "outputId": "3a7f97e2-b621-44c8-d7ee-0670c2b9e0ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/fish-speech\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: transformers>=4.45.2 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (4.48.3)\n",
            "Collecting datasets==2.18.0 (from fish-speech==0.1.0)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lightning>=2.1.0 (from fish-speech==0.1.0)\n",
            "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.3.2 (from fish-speech==0.1.0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: tensorboard>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: natsort>=8.4.0 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (8.4.0)\n",
            "Requirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (0.8.1)\n",
            "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (0.10.2.post1)\n",
            "Requirement already satisfied: rich>=13.5.3 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (13.9.4)\n",
            "Collecting gradio>5.0.0 (from fish-speech==0.1.0)\n",
            "  Downloading gradio-5.20.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: wandb>=0.15.11 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (0.19.8)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (1.70.0)\n",
            "Collecting kui>=1.6.0 (from fish-speech==0.1.0)\n",
            "  Downloading kui-1.8.1-py3-none-any.whl.metadata (984 bytes)\n",
            "Collecting uvicorn>=0.30.0 (from fish-speech==0.1.0)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting loguru>=0.6.0 (from fish-speech==0.1.0)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting loralib>=0.1.2 (from fish-speech==0.1.0)\n",
            "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyrootutils>=1.0.4 (from fish-speech==0.1.0)\n",
            "  Downloading pyrootutils-1.0.4-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting vector_quantize_pytorch==1.14.24 (from fish-speech==0.1.0)\n",
            "  Downloading vector_quantize_pytorch-1.14.24-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting resampy>=0.4.3 (from fish-speech==0.1.0)\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting einx==0.2.2 (from einx[torch]==0.2.2->fish-speech==0.1.0)\n",
            "  Downloading einx-0.2.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: zstandard>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (0.23.0)\n",
            "Collecting pydub (from fish-speech==0.1.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyaudio (from fish-speech==0.1.0)\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faster_whisper (from fish-speech==0.1.0)\n",
            "  Downloading faster_whisper-1.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting modelscope==1.17.1 (from fish-speech==0.1.0)\n",
            "  Downloading modelscope-1.17.1-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting funasr==1.1.5 (from fish-speech==0.1.0)\n",
            "  Downloading funasr-1.1.5-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting opencc-python-reimplemented==0.1.7 (from fish-speech==0.1.0)\n",
            "  Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting silero-vad (from fish-speech==0.1.0)\n",
            "  Downloading silero_vad-5.1.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting ormsgpack (from fish-speech==0.1.0)\n",
            "  Downloading ormsgpack-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken>=0.8.0 (from fish-speech==0.1.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting pydantic==2.9.2 (from fish-speech==0.1.0)\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from fish-speech==0.1.0) (5.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.18.0->fish-speech==0.1.0)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.18.0->fish-speech==0.1.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (4.67.1)\n",
            "Collecting xxhash (from datasets==2.18.0->fish-speech==0.1.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets==2.18.0->fish-speech==0.1.0)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.2.0,>=2023.1.0 (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->fish-speech==0.1.0)\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.18.0->fish-speech==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from einx==0.2.2->einx[torch]==0.2.2->fish-speech==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx==0.2.2->einx[torch]==0.2.2->fish-speech==0.1.0) (2.4.6)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from einx[torch]==0.2.2->fish-speech==0.1.0) (2.4.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from funasr==1.1.5->fish-speech==0.1.0) (1.14.1)\n",
            "Collecting jamo (from funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from funasr==1.1.5->fish-speech==0.1.0) (0.13.1)\n",
            "Collecting kaldiio>=2.17.0 (from funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading kaldiio-2.18.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting torch-complex (from funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading torch_complex-0.4.4-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from funasr==1.1.5->fish-speech==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from funasr==1.1.5->fish-speech==0.1.0) (0.42.1)\n",
            "Collecting pytorch-wpe (from funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading pytorch_wpe-0.0.1-py3-none-any.whl.metadata (242 bytes)\n",
            "Requirement already satisfied: editdistance>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from funasr==1.1.5->fish-speech==0.1.0) (0.8.1)\n",
            "Collecting oss2 (from funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading oss2-2.19.1.tar.gz (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (from funasr==1.1.5->fish-speech==0.1.0) (0.5.7)\n",
            "Collecting jaconv (from funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading jaconv-0.4.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorboardX (from funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope==1.17.1->fish-speech==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.9.2->fish-speech==0.1.0) (0.7.0)\n",
            "Collecting pydantic-core==2.23.4 (from pydantic==2.9.2->fish-speech==0.1.0)\n",
            "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.9.2->fish-speech==0.1.0) (4.12.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>5.0.0->fish-speech==0.1.0) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio>5.0.0->fish-speech==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio>5.0.0->fish-speech==0.1.0) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio>5.0.0->fish-speech==0.1.0) (3.10.15)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio>5.0.0->fish-speech==0.1.0) (11.1.0)\n",
            "Collecting python-multipart>=0.0.18 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio>5.0.0->fish-speech==0.1.0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio>5.0.0->fish-speech==0.1.0) (0.15.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio>5.0.0->fish-speech==0.1.0) (14.2)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->fish-speech==0.1.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->fish-speech==0.1.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting baize>=0.20.0 (from kui>=1.6.0->fish-speech==0.1.0)\n",
            "  Downloading baize-0.22.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.1->fish-speech==0.1.0) (1.1.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.1.0->fish-speech==0.1.0)\n",
            "  Downloading lightning_utilities-0.14.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning>=2.1.0->fish-speech==0.1.0)\n",
            "  Downloading torchmetrics-1.6.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.1.0->fish-speech==0.1.0)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting python-dotenv>=0.20.0 (from pyrootutils>=1.0.4->fish-speech==0.1.0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.5.3->fish-speech==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.5.3->fish-speech==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.1->fish-speech==0.1.0) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.1->fish-speech==0.1.0) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.1->fish-speech==0.1.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.1->fish-speech==0.1.0) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.1->fish-speech==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.1->fish-speech==0.1.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.14.1->fish-speech==0.1.0) (3.1.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.8.0->fish-speech==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.2->fish-speech==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.2->fish-speech==0.1.0) (0.5.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.0->fish-speech==0.1.0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.30.0->fish-speech==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.11->fish-speech==0.1.0) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.11->fish-speech==0.1.0) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.11->fish-speech==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.11->fish-speech==0.1.0) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.11->fish-speech==0.1.0) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.15.11->fish-speech==0.1.0) (1.3.5)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster_whisper->fish-speech==0.1.0)\n",
            "  Downloading ctranslate2-4.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster_whisper->fish-speech==0.1.0)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting av>=11 (from faster_whisper->fish-speech==0.1.0)\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: torchaudio>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from silero-vad->fish-speech==0.1.0) (2.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>5.0.0->fish-speech==0.1.0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio>5.0.0->fish-speech==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0->fish-speech==0.1.0) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0->fish-speech==0.1.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0->fish-speech==0.1.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0->fish-speech==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0->fish-speech==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0->fish-speech==0.1.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.18.0->fish-speech==0.1.0) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.15.11->fish-speech==0.1.0) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio>5.0.0->fish-speech==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio>5.0.0->fish-speech==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.3->fish-speech==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa>=0.10.1->fish-speech==0.1.0) (0.43.0)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster_whisper->fish-speech==0.1.0)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster_whisper->fish-speech==0.1.0) (25.2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0->fish-speech==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0->fish-speech==0.1.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.18.0->fish-speech==0.1.0) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.18.0->fish-speech==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa>=0.10.1->fish-speech==0.1.0) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->funasr==1.1.5->fish-speech==0.1.0) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->einx[torch]==0.2.2->fish-speech==0.1.0) (12.5.82)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio>5.0.0->fish-speech==0.1.0) (1.5.4)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets==2.18.0->fish-speech==0.1.0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting crcmod>=1.7 (from oss2->funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2->funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->einx==0.2.2->einx[torch]==0.2.2->fish-speech==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn->funasr==1.1.5->fish-speech==0.1.0) (0.5.13)\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.1.5->fish-speech==0.1.0)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.1.5->fish-speech==0.1.0) (43.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->funasr==1.1.5->fish-speech==0.1.0) (2.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.15.11->fish-speech==0.1.0) (5.0.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster_whisper->fish-speech==0.1.0)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einx-0.2.2-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funasr-1.1.5-py3-none-any.whl (649 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m649.0/649.0 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.17.1-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vector_quantize_pytorch-1.14.24-py3-none-any.whl (36 kB)\n",
            "Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.20.1-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kui-1.8.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
            "Downloading pyrootutils-1.0.4-py3-none-any.whl (5.8 kB)\n",
            "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faster_whisper-1.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.1/223.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading silero_vad-5.1.2-py3-none-any.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading baize-0.22.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (733 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.9/733.9 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading kaldiio-2.18.1-py3-none-any.whl (29 kB)\n",
            "Downloading lightning_utilities-0.14.0-py3-none-any.whl (28 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading torchmetrics-1.6.2-py3-none-any.whl (931 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m931.6/931.6 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_complex-0.4.4-py3-none-any.whl (9.1 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fish-speech, antlr4-python3-runtime, pyaudio, jaconv, oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building editable for fish-speech (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fish-speech: filename=fish_speech-0.1.0-0.editable-py3-none-any.whl size=10341 sha256=30c8e118f8473212439bd17670cacd49a4e3714ff7b086177c9792ce4bca3cb0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3p9x8v14/wheels/1d/8d/79/64aa115a26c1628f19c01f0b6214e49c83c75365353fa810b5\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=b5aa886918e0e63e98a4fc8ef727c6c49f18bc1c414ecb0bd50dd40980c0a69f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=pyaudio-0.2.14-cp311-cp311-linux_x86_64.whl size=67394 sha256=ccf5d284ce5ea39dcbd76f9c3b8a030fe0d7a08f3fab91a587a0aa724a69b044\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/b1/c1/67e4ef443de2665d86031d4760508094eab5de37d5d64d9c27\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.4.0-py3-none-any.whl size=18229 sha256=c799ed0c1f4f53abc9ad22ce14e94bd586b25a2779c74e20ddb28e41506d7b59\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/63/71/95fb322fe9047ed7e61b007c47cbf03d23ecb77dd03665f151\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.19.1-py3-none-any.whl size=123943 sha256=ec1d0747d81b24d833676d568763b962f0986b22cc121eebd9d42aa751fe8524\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/27/a3/50e7db0dd68810d9d4e383a547b88b4a5b1eaae58e63c1d64a\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535316 sha256=f24edebd3bb191091e015a4459bb93878b20391dfd6748072d7af79a9987a8d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/9a/95/60f111d2a488c5f7f7ed2a96ce407ea57ec7393ddfdec8c956\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp311-cp311-linux_x86_64.whl size=31658 sha256=bdf76c46ddf30093f132042f995eebe73e4b08638271b36073ef52f0e39cd5cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99\n",
            "Successfully built fish-speech antlr4-python3-runtime pyaudio jaconv oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: pydub, pyaudio, opencc-python-reimplemented, jamo, jaconv, crcmod, antlr4-python3-runtime, xxhash, uvicorn, torch-complex, tomlkit, tensorboardX, semantic-version, ruff, pytorch-wpe, python-multipart, python-dotenv, pydantic-core, pycryptodome, pyarrow-hotfix, ormsgpack, omegaconf, markupsafe, loralib, loguru, lightning-utilities, kaldiio, jmespath, humanfriendly, groovy, fsspec, ffmpy, dill, ctranslate2, baize, av, aiofiles, tiktoken, starlette, resampy, pyrootutils, pydantic, multiprocess, modelscope, hydra-core, einx, coloredlogs, safehttpx, onnxruntime, kui, gradio-client, fastapi, aliyun-python-sdk-core, vector_quantize_pytorch, torchmetrics, gradio, faster_whisper, datasets, aliyun-python-sdk-kms, silero-vad, pytorch-lightning, oss2, lightning, funasr, fish-speech\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.6\n",
            "    Uninstalling pydantic-2.10.6:\n",
            "      Successfully uninstalled pydantic-2.10.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 antlr4-python3-runtime-4.9.3 av-14.2.0 baize-0.22.2 coloredlogs-15.0.1 crcmod-1.7 ctranslate2-4.5.0 datasets-2.18.0 dill-0.3.8 einx-0.2.2 fastapi-0.115.11 faster_whisper-1.1.1 ffmpy-0.5.0 fish-speech-0.1.0 fsspec-2024.2.0 funasr-1.1.5 gradio-5.20.1 gradio-client-1.7.2 groovy-0.1.2 humanfriendly-10.0 hydra-core-1.3.2 jaconv-0.4.0 jamo-0.4.1 jmespath-0.10.0 kaldiio-2.18.1 kui-1.8.1 lightning-2.5.0.post0 lightning-utilities-0.14.0 loguru-0.7.3 loralib-0.1.2 markupsafe-2.1.5 modelscope-1.17.1 multiprocess-0.70.16 omegaconf-2.3.0 onnxruntime-1.21.0 opencc-python-reimplemented-0.1.7 ormsgpack-1.8.0 oss2-2.19.1 pyarrow-hotfix-0.6 pyaudio-0.2.14 pycryptodome-3.21.0 pydantic-2.9.2 pydantic-core-2.23.4 pydub-0.25.1 pyrootutils-1.0.4 python-dotenv-1.0.1 python-multipart-0.0.20 pytorch-lightning-2.5.0.post0 pytorch-wpe-0.0.1 resampy-0.4.3 ruff-0.9.10 safehttpx-0.1.6 semantic-version-2.10.0 silero-vad-5.1.2 starlette-0.46.1 tensorboardX-2.6.2.2 tiktoken-0.9.0 tomlkit-0.13.2 torch-complex-0.4.4 torchmetrics-1.6.2 uvicorn-0.34.0 vector_quantize_pytorch-1.14.24 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "6f6f30501c244a99a1918cc34993e8ab"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4NGqj8VKddef",
        "outputId": "c094d200-6701-4879-a04d-7e185c152210"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en_US.UTF-8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import locale\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfV_Jt6iddeg"
      },
      "source": [
        "### Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-NpQ8fUddeg",
        "outputId": "ad05404c-78ed-4e49-8be2-88abfd3e7cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 7 files:   0% 0/7 [00:00<?, ?it/s]Downloading 'special_tokens.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Pdr1pnDFqf3r8xSTD-lPnaCpeRA=.db54e3cccbbaa1106ba8d56e810dffd42e325ab0.incomplete'\n",
            "Downloading 'model.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/YT0Y2lJH9mHYafdr2d9j82hXvzY=.918dc960372cc1b77bbafb14c48ef7a1634ecf75d4eb85b78607223b780d6001.incomplete'\n",
            "\n",
            "model.pth:   0% 0.00/1.28G [00:00<?, ?B/s]\u001b[ADownloading 'config.json' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/8_PA_wEVGiVa2goH2H4KQOQpvVY=.3f8edf91f7a0b152e5f8c30fd412c5d7e22020b5.incomplete'\n",
            "\n",
            "\n",
            "special_tokens.json: 100% 31.0k/31.0k [00:00<00:00, 2.03MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/special_tokens.json\n",
            "\n",
            "\n",
            "config.json: 100% 697/697 [00:00<00:00, 5.70MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/config.json\n",
            "\n",
            "model.pth:   2% 31.5M/1.28G [00:00<00:05, 236MB/s]\u001b[ADownloading '.gitattributes' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'tokenizer.tiktoken' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/zENsYUfT6EG2Nj68LEJ8oOfAxB8=.21dcfcb37df8da533b2d4fe0b867472f04cda62e.incomplete'\n",
            "Downloading 'firefly-gan-vq-fsq-8x1024-21hz-generator.pth' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Khmizewsuzbxb3XfvhhbrTGaoLE=.01b81dbf753224a156c3fe139b88bf0b9a0f54b11bee864f95e66511c3ccd754.incomplete'\n",
            "Downloading 'README.md' to 'checkpoints/fish-speech-1.5/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.a43b65346beef6fa83135bd1dff857a00b6a9c13.incomplete'\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:   0% 0.00/189M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "model.pth:   5% 62.9M/1.28G [00:00<00:04, 248MB/s]\u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:   6% 10.5M/189M [00:00<00:02, 81.5MB/s]\u001b[A\u001b[A\n",
            "model.pth:   7% 94.4M/1.28G [00:00<00:04, 239MB/s]\u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  17% 31.5M/189M [00:00<00:01, 146MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 9.97MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/.gitattributes\n",
            "Fetching 7 files:  14% 1/7 [00:00<00:04,  1.47it/s]\n",
            "\n",
            "\n",
            "README.md: 100% 1.68k/1.68k [00:00<00:00, 11.6MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/README.md\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.tiktoken:   0% 0.00/1.70M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  28% 52.4M/189M [00:00<00:00, 149MB/s]\u001b[A\u001b[A\n",
            "model.pth:  10% 126M/1.28G [00:00<00:05, 205MB/s] \u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  39% 73.4M/189M [00:00<00:00, 147MB/s]\u001b[A\u001b[A\n",
            "model.pth:  12% 157M/1.28G [00:00<00:06, 183MB/s]\u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  50% 94.4M/189M [00:00<00:00, 165MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.tiktoken: 100% 1.70M/1.70M [00:00<00:00, 4.35MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/tokenizer.tiktoken\n",
            "\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  61% 115M/189M [00:00<00:00, 173MB/s] \u001b[A\u001b[A\n",
            "model.pth:  14% 178M/1.28G [00:00<00:06, 172MB/s]\u001b[A\n",
            "model.pth:  16% 199M/1.28G [00:01<00:06, 177MB/s]\u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  72% 136M/189M [00:00<00:00, 170MB/s]\u001b[A\u001b[A\n",
            "model.pth:  17% 220M/1.28G [00:01<00:05, 181MB/s]\u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  83% 157M/189M [00:00<00:00, 171MB/s]\u001b[A\u001b[A\n",
            "\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth:  95% 178M/189M [00:01<00:00, 179MB/s]\u001b[A\u001b[A\n",
            "(…)fly-gan-vq-fsq-8x1024-21hz-generator.pth: 100% 189M/189M [00:01<00:00, 162MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\n",
            "Fetching 7 files:  57% 4/7 [00:01<00:01,  2.76it/s]\n",
            "model.pth:  21% 262M/1.28G [00:01<00:05, 177MB/s]\u001b[A\n",
            "model.pth:  24% 304M/1.28G [00:01<00:04, 208MB/s]\u001b[A\n",
            "model.pth:  26% 336M/1.28G [00:01<00:04, 221MB/s]\u001b[A\n",
            "model.pth:  29% 367M/1.28G [00:01<00:03, 232MB/s]\u001b[A\n",
            "model.pth:  31% 398M/1.28G [00:01<00:03, 226MB/s]\u001b[A\n",
            "model.pth:  34% 430M/1.28G [00:02<00:03, 235MB/s]\u001b[A\n",
            "model.pth:  36% 461M/1.28G [00:02<00:03, 225MB/s]\u001b[A\n",
            "model.pth:  39% 493M/1.28G [00:02<00:03, 225MB/s]\u001b[A\n",
            "model.pth:  41% 524M/1.28G [00:02<00:03, 226MB/s]\u001b[A\n",
            "model.pth:  44% 556M/1.28G [00:02<00:03, 228MB/s]\u001b[A\n",
            "model.pth:  46% 587M/1.28G [00:02<00:02, 231MB/s]\u001b[A\n",
            "model.pth:  48% 619M/1.28G [00:02<00:02, 237MB/s]\u001b[A\n",
            "model.pth:  51% 650M/1.28G [00:02<00:02, 252MB/s]\u001b[A\n",
            "model.pth:  53% 682M/1.28G [00:03<00:02, 253MB/s]\u001b[A\n",
            "model.pth:  56% 713M/1.28G [00:03<00:02, 255MB/s]\u001b[A\n",
            "model.pth:  58% 744M/1.28G [00:03<00:02, 253MB/s]\u001b[A\n",
            "model.pth:  61% 776M/1.28G [00:03<00:01, 254MB/s]\u001b[A\n",
            "model.pth:  63% 807M/1.28G [00:03<00:01, 256MB/s]\u001b[A\n",
            "model.pth:  66% 839M/1.28G [00:03<00:01, 252MB/s]\u001b[A\n",
            "model.pth:  68% 870M/1.28G [00:03<00:01, 248MB/s]\u001b[A\n",
            "model.pth:  71% 902M/1.28G [00:03<00:01, 252MB/s]\u001b[A\n",
            "model.pth:  73% 933M/1.28G [00:04<00:01, 231MB/s]\u001b[A\n",
            "model.pth:  76% 965M/1.28G [00:04<00:01, 226MB/s]\u001b[A\n",
            "model.pth:  78% 996M/1.28G [00:04<00:01, 228MB/s]\u001b[A\n",
            "model.pth:  81% 1.03G/1.28G [00:04<00:01, 234MB/s]\u001b[A\n",
            "model.pth:  83% 1.06G/1.28G [00:04<00:00, 224MB/s]\u001b[A\n",
            "model.pth:  85% 1.09G/1.28G [00:04<00:00, 229MB/s]\u001b[A\n",
            "model.pth:  88% 1.12G/1.28G [00:04<00:00, 231MB/s]\u001b[A\n",
            "model.pth:  90% 1.15G/1.28G [00:05<00:00, 239MB/s]\u001b[A\n",
            "model.pth:  93% 1.18G/1.28G [00:05<00:00, 250MB/s]\u001b[A\n",
            "model.pth:  95% 1.22G/1.28G [00:05<00:00, 235MB/s]\u001b[A\n",
            "model.pth:  98% 1.25G/1.28G [00:05<00:00, 237MB/s]\u001b[A\n",
            "model.pth: 100% 1.28G/1.28G [00:05<00:00, 226MB/s]\n",
            "Download complete. Moving file to checkpoints/fish-speech-1.5/model.pth\n",
            "Fetching 7 files: 100% 7/7 [00:05<00:00,  1.20it/s]\n",
            "/content/checkpoints/fish-speech-1.5\n"
          ]
        }
      ],
      "source": [
        "# For Chinese users, you probably want to use mirror to accelerate downloading\n",
        "# !set HF_ENDPOINT=https://hf-mirror.com\n",
        "# !export HF_ENDPOINT=https://hf-mirror.com\n",
        "\n",
        "!huggingface-cli download fishaudio/fish-speech-1.5 --local-dir checkpoints/fish-speech-1.5/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XGLChtsddeg"
      },
      "source": [
        "## WebUI Inference\n",
        "\n",
        "> You can use --compile to fuse CUDA kernels for faster inference (10x)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu8rKLQKddeh",
        "outputId": "bfb48d24-7baf-4cd3-af12-39b72f2e22fd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2025-03-05 04:42:51.728\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mLoading Llama model...\u001b[0m\n",
            "\u001b[32m2025-03-05 04:43:04.017\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mRestored model from checkpoint\u001b[0m\n",
            "\u001b[32m2025-03-05 04:43:04.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m687\u001b[0m - \u001b[1mUsing DualARTransformer\u001b[0m\n",
            "\u001b[32m2025-03-05 04:43:04.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m695\u001b[0m - \u001b[1mCompiling function...\u001b[0m\n",
            "\u001b[32m2025-03-05 04:43:05.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mLoading VQ-GAN model...\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/finite_scalar_quantization.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/lookup_free_quantization.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "\u001b[32m2025-03-05 04:43:08.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.vqgan.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
            "\u001b[32m2025-03-05 04:43:08.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mDecoder model loaded, warming up...\u001b[0m\n",
            "\u001b[32m2025-03-05 04:43:08.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hello world.\u001b[0m\n",
            "\u001b[32m2025-03-05 04:43:08.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
            "  0% 0/1023 [00:00<?, ?it/s]/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "W0305 04:44:58.846000 133912790808128 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
            "  0% 1/1023 [04:49<82:11:39, 289.53s/it]/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "  0% 2/1023 [04:50<33:59:24, 119.85s/it]/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "  2% 19/1023 [04:50<4:16:07, 15.31s/it]\n",
            "\u001b[32m2025-03-05 04:48:00.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 291.44 seconds\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:00.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 21 tokens in 291.44 seconds, 0.07 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:00.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 0.05 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:00.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.22 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:00.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 20])\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:02.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mWarming up done, launching the web UI...\u001b[0m\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://92efef341e737c722b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "\u001b[32m2025-03-05 04:48:24.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 臣司马光曰：\n",
            "　　我知道天子的职责中最重要的是维护礼教，礼教中最重要的是区分地位，区分地位中最重要的是匡正名分。什么是礼教？就是法纪。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:24.276\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 什么是区分地位？就是君臣有别。什么是名分？就是公、侯、卿、大夫等官爵。\n",
            "　　四海之广，亿民之众，都受制于天子一人。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:24.286\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 尽管是才能超群、智慧绝伦的人，也不能不在天子足下为他奔走服务，这难道不是以礼作为礼纪朝纲的作用吗！\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:24.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 所以，天子统率三公，三公督率诸侯国君，诸侯国君节制卿、大夫官员，卿、大夫官员又统治士人百姓。权贵支配贱民，贱民服从权贵。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:24.307\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 上层指挥下层就好像人的心腹控制四肢行动，树木的根和干支配枝和叶；下层服侍上层就好像人的四肢卫护心腹，树木的枝和叶遮护根和干，\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:24.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 这样才能上下层互相保护，从而使国家得到长治久安。所以说，天子的职责没有比维护礼制更重要的了。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:24.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/6 of sample 1/1\u001b[0m\n",
            "  0% 0/8129 [00:00<?, ?it/s]/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "  4% 339/8129 [00:04<01:33, 83.45it/s]\n",
            "\u001b[32m2025-03-05 04:48:28.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 4.20 seconds\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:28.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 341 tokens in 4.20 seconds, 81.16 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:28.520\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 51.77 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:28.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.26 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:28.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:28.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 340])\u001b[0m\n",
            "  4% 283/7735 [00:03<01:31, 81.75it/s]\n",
            "\u001b[32m2025-03-05 04:48:32.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 285 tokens in 3.72 seconds, 76.64 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:32.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 48.89 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:32.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.63 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:32.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:32.242\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 284])\u001b[0m\n",
            "  3% 204/7408 [00:02<01:27, 81.90it/s]\n",
            "\u001b[32m2025-03-05 04:48:35.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 206 tokens in 2.77 seconds, 74.32 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:35.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 47.41 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:35.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.63 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:35.014\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:35.015\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 205])\u001b[0m\n",
            "  4% 295/7148 [00:03<01:24, 81.55it/s]\n",
            "\u001b[32m2025-03-05 04:48:38.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 297 tokens in 3.94 seconds, 75.44 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:38.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 48.13 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:38.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.63 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:38.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:38.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 296])\u001b[0m\n",
            "  4% 300/6799 [00:03<01:16, 84.55it/s]\n",
            "\u001b[32m2025-03-05 04:48:42.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 302 tokens in 4.00 seconds, 75.48 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:42.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 48.15 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:42.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.63 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:42.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:42.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 301])\u001b[0m\n",
            "  3% 193/6460 [00:02<01:14, 84.10it/s]\n",
            "\u001b[32m2025-03-05 04:48:45.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 195 tokens in 2.80 seconds, 69.56 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:45.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 44.38 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:45.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.63 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:48:45.760\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 194])\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py:749: UserWarning: Trying to convert audio automatically from float32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "\u001b[32m2025-03-05 04:59:20.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mLoaded audio with 19.54 seconds\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/residual_fsq.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled = False):\n",
            "\u001b[32m2025-03-05 04:59:21.112\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mencode_reference\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mEncoded prompt: torch.Size([8, 421])\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:21.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 臣司马光曰：\n",
            "　　我知道天子的职责中最重要的是维护礼教，礼教中最重要的是区分地位，区分地位中最重要的是匡正名分。什么是礼教？就是法纪。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:21.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 什么是区分地位？就是君臣有别。什么是名分？就是公、侯、卿、大夫等官爵。\n",
            "　　四海之广，亿民之众，都受制于天子一人。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:21.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 尽管是才能超群、智慧绝伦的人，也不能不在天子足下为他奔走服务，这难道不是以礼作为礼纪朝纲的作用吗！\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:21.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 所以，天子统率三公，三公督率诸侯国君，诸侯国君节制卿、大夫官员，卿、大夫官员又统治士人百姓。权贵支配贱民，贱民服从权贵。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:21.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 上层指挥下层就好像人的心腹控制四肢行动，树木的根和干支配枝和叶；下层服侍上层就好像人的四肢卫护心腹，树木的枝和叶遮护根和干，\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:21.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: 这样才能上下层互相保护，从而使国家得到长治久安。所以说，天子的职责没有比维护礼制更重要的了。\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:21.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/6 of sample 1/1\u001b[0m\n",
            "  4% 343/7698 [00:04<01:26, 84.76it/s]\n",
            "\u001b[32m2025-03-05 04:59:25.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 4.28 seconds\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:25.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 345 tokens in 4.28 seconds, 80.69 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:25.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 51.47 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:25.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.63 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:25.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 2/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:25.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 344])\u001b[0m\n",
            "  9% 664/7300 [00:07<01:18, 84.92it/s]\n",
            "\u001b[32m2025-03-05 04:59:33.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 666 tokens in 8.24 seconds, 80.85 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:33.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 51.57 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:33.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.67 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:33.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 3/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 04:59:33.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 665])\u001b[0m\n",
            "100% 6592/6592 [01:18<00:00, 83.80it/s]\n",
            "\u001b[32m2025-03-05 05:00:53.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 6593 tokens in 79.38 seconds, 83.05 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 05:00:53.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 52.98 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 05:00:53.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 3.07 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 05:00:53.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 4/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 05:00:53.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 6592])\u001b[0m\n",
            "  5% 354/7299 [00:04<01:23, 83.05it/s]\n",
            "\u001b[32m2025-03-05 05:01:01.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 356 tokens in 8.85 seconds, 40.24 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:01.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 25.67 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:01.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 10.16 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:01.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 5/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:01.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 355])\u001b[0m\n",
            "  5% 365/6891 [00:04<01:18, 82.93it/s]\n",
            "\u001b[32m2025-03-05 05:01:06.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 367 tokens in 4.91 seconds, 74.82 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:06.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 47.73 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:06.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 10.16 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:06.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 6/6 of sample 1/1\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:06.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 366])\u001b[0m\n",
            "  4% 236/6487 [00:02<01:16, 81.43it/s]\n",
            "\u001b[32m2025-03-05 05:01:10.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 238 tokens in 3.47 seconds, 68.52 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:10.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 43.71 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:10.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 10.16 GB\u001b[0m\n",
            "\u001b[32m2025-03-05 05:01:10.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 237])\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python tools/run_webui.py \\\n",
        "    --llama-checkpoint-path checkpoints/fish-speech-1.5 \\\n",
        "    --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth \\\n",
        "    --compile \\\n",
        "    --half"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "0TA9uTuQHBra",
        "outputId": "acfaaec8-947e-4bb9-b1e4-89fda413ef98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m tools.api_server \\\n",
        "    --listen 0.0.0.0:6666 \\\n",
        "    --llama-checkpoint-path \"checkpoints/fish-speech-1.5\" \\\n",
        "    --decoder-checkpoint-path \"checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth\" \\\n",
        "    --decoder-config-name firefly_gan_vq \\\n",
        "    --compile \\\n",
        "    --half"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrMI0kQQGoXF",
        "outputId": "032fb53d-ce3d-4926-eb0a-ffd6878584a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * ngrok tunnel \"https://1752-35-204-121-166.ngrok-free.app\" -> \"http://127.0.0.1:8080\"\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m5635\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32m2025-03-13 01:27:07.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mRestored model from checkpoint\u001b[0m\n",
            "\u001b[32m2025-03-13 01:27:07.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m687\u001b[0m - \u001b[1mUsing DualARTransformer\u001b[0m\n",
            "\u001b[32m2025-03-13 01:27:07.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m695\u001b[0m - \u001b[1mCompiling function...\u001b[0m\n",
            "\u001b[32m2025-03-13 01:27:08.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.server.model_manager\u001b[0m:\u001b[36mload_llama_model\u001b[0m:\u001b[36m99\u001b[0m - \u001b[1mLLAMA model loaded.\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:630: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/finite_scalar_quantization.py:147: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "/usr/local/lib/python3.11/dist-packages/vector_quantize_pytorch/lookup_free_quantization.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(enabled = False)\n",
            "\u001b[32m2025-03-13 01:27:11.908\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.vqgan.inference\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mLoaded model: <All keys matched successfully>\u001b[0m\n",
            "\u001b[32m2025-03-13 01:27:11.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.server.model_manager\u001b[0m:\u001b[36mload_decoder_model\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1mDecoder model loaded.\u001b[0m\n",
            "\u001b[32m2025-03-13 01:27:11.955\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mEncoded text: Hello world.\u001b[0m\n",
            "\u001b[32m2025-03-13 01:27:11.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m806\u001b[0m - \u001b[1mGenerating sentence 1/1 of sample 1/1\u001b[0m\n",
            "  0% 0/1023 [00:00<?, ?it/s]/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "W0313 01:29:02.741000 138679953647168 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
            "  0% 1/1023 [04:50<82:24:19, 290.27s/it]/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "  0% 2/1023 [04:51<34:06:59, 120.29s/it]/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
            "  self.gen = func(*args, **kwds)\n",
            "  2% 22/1023 [04:51<3:41:21, 13.27s/it]\n",
            "\u001b[32m2025-03-13 01:32:06.208\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m851\u001b[0m - \u001b[1mCompilation time: 294.23 seconds\u001b[0m\n",
            "\u001b[32m2025-03-13 01:32:06.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m860\u001b[0m - \u001b[1mGenerated 24 tokens in 294.23 seconds, 0.08 tokens/sec\u001b[0m\n",
            "\u001b[32m2025-03-13 01:32:06.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m863\u001b[0m - \u001b[1mBandwidth achieved: 0.05 GB/s\u001b[0m\n",
            "\u001b[32m2025-03-13 01:32:06.209\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.models.text2semantic.inference\u001b[0m:\u001b[36mgenerate_long\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mGPU Memory used: 2.22 GB\u001b[0m\n",
            "\u001b[32m2025-03-13 01:32:06.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mfish_speech.inference_engine.vq_manager\u001b[0m:\u001b[36mdecode_vq_tokens\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mVQ features: torch.Size([8, 23])\u001b[0m\n",
            "\u001b[32m2025-03-13 01:32:08.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtools.server.model_manager\u001b[0m:\u001b[36mwarm_up\u001b[0m:\u001b[36m122\u001b[0m - \u001b[1mModels warmed up.\u001b[0m\n",
            "\u001b[32m2025-03-13 01:32:08.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minitialize_app\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStartup done, listening server at http://0.0.0.0:8080\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[31mERROR\u001b[0m:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8080): address already in use\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "WARNING:pyngrok.process.ngrok:t=2025-03-13T01:32:08+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8080-caf4d231-2bfa-466f-aa9a-b1c93c969de7 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-03-13T01:32:08+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8080-caf4d231-2bfa-466f-aa9a-b1c93c969de7 err=\"failed to start tunnel: session closed\"\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}